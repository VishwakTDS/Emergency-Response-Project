{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "ruCZnw8xeNTu",
        "outputId": "92cec7ac-225c-4791-f18f-c4812f0939af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-nvidia-ai-endpoints==0.3.5\n",
            "  Downloading langchain_nvidia_ai_endpoints-0.3.5-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from langchain-nvidia-ai-endpoints==0.3.5) (3.11.15)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-nvidia-ai-endpoints==0.3.5) (0.3.65)\n",
            "Collecting pillow<11.0.0,>=10.0.0 (from langchain-nvidia-ai-endpoints==0.3.5)\n",
            "  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints==0.3.5) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints==0.3.5) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints==0.3.5) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints==0.3.5) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints==0.3.5) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints==0.3.5) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints==0.3.5) (1.20.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (0.3.45)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (4.14.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (2.11.7)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (3.10.18)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (0.4.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints==0.3.5) (3.10)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.3.45->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.3.45->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints==0.3.5) (1.3.1)\n",
            "Downloading langchain_nvidia_ai_endpoints-0.3.5-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.1/41.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pillow, langchain-nvidia-ai-endpoints\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.2.1\n",
            "    Uninstalling pillow-11.2.1:\n",
            "      Successfully uninstalled pillow-11.2.1\n",
            "Successfully installed langchain-nvidia-ai-endpoints-0.3.5 pillow-10.4.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "885afd49f3bf4f3aa4b4340773c5b8de",
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting psycopg2-binary\n",
            "  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-nvidia-ai-endpoints==0.3.5\n",
        "!pip install psycopg2-binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHyNq7g9d5RR"
      },
      "outputs": [],
      "source": [
        "import os, re, sqlite3, requests, json, numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "from dateutil import parser\n",
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings, NVIDIARerank\n",
        "from langchain.schema import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CYsdXwcgOFk"
      },
      "outputs": [],
      "source": [
        "os.environ[\"NVIDIA_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spJcY4171_yw"
      },
      "source": [
        "# List of incidents and corresponding news article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyOqNNH6eKDU"
      },
      "outputs": [],
      "source": [
        "incidents = {\n",
        "    \"Oregon_Rowena_Fire_Jun2025\": [\n",
        "       'https://apnews.com/article/oregon-wildfire-interstate-84-8d28833059cd46878080fd7e71444548',\n",
        "       'https://www.opb.org/article/2025/05/14/an-architect-of-oregons-wildfire-map-on-why-he-now-supports-repealing-it/',\n",
        "       'https://www.kgw.com/article/news/local/wildfire/oregon-state-fire-marshal-clears-misinformation/283-49abc8db-7d30-4bd9-8b5d-af82a6a3cdc7',\n",
        "       'https://www.eenews.net/articles/oregon-lawmakers-ready-to-junk-contentious-wildfire-map/',\n",
        "       'https://www.theguardian.com/us-news/article/2024/jul/27/oregon-wildfire-season',\n",
        "       'https://www.opb.org/article/2025/05/24/oregon-fire-restrictions-washington-wildfire-bureau-land-management/',\n",
        "       'https://apnews.com/article/oregon-wildfire-hazard-map-45c0335d93632580e07512a276dea7da',\n",
        "       'https://www.kgw.com/article/news/local/wildfire/oregon-firefighters-return-north-carolina/283-7e13e4fc-10ad-498a-b4cb-85a5b0845603'\n",
        "    ],\n",
        "    \"Maui_Kahikinui_Fire_Jun2025\": [\n",
        "      \"https://www.theguardian.com/us-news/2025/jun/17/hawaii-maui-wildfire\",\n",
        "      'https://www.foxweather.com/extreme-weather/maui-brush-fire-kahikinui-hawaii-evacuations',\n",
        "      'https://www.kitv.com/news/local/maui-county-hands-over-kahikinui-fire-control-to-state-of-hawaii/article_e453a5ab-90be-4d81-93d1-aeee87e5f989.html',\n",
        "      'https://www.butlereagle.com/20250616/fast-moving-brush-fire-on-hawaiis-maui-island-evacuates-about-50-people-no-structures-have-burned/',\n",
        "      'https://www.kcra.com/article/fast-moving-brush-fire-in-hawaiis-maui-county-evacuates-at-least-105-homes-no-structures-burned/65078861'\n",
        "    ]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF-98aO72Ncv"
      },
      "source": [
        "# Web scraper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLUgMpMWestm"
      },
      "outputs": [],
      "source": [
        "def extract_date(soup):\n",
        "    selectors = [\n",
        "      ('meta',{'itemprop':'datePublished'}),('meta',{'name':'pubdate'}),\n",
        "      ('meta',{'name':'date'}),('meta',{'property':'article:published_time'}),\n",
        "      ('span',{'class':'update-time'}),('p',{'class':'update-time'})\n",
        "    ]\n",
        "    for tag,attrs in selectors:\n",
        "        el=soup.find(tag,attrs=attrs)\n",
        "        if not el: continue\n",
        "        s = el.get('content') if tag=='meta' and el.has_attr('content') else el.get_text(strip=True)\n",
        "        try: return parser.parse(s,fuzzy=True).strftime('%Y-%m-%d')\n",
        "        except: return s\n",
        "    return \"n/a\"\n",
        "\n",
        "def scrape_article(url):\n",
        "    r = requests.get(url,headers={\"User-Agent\":\"Mozilla/5.0\"},timeout=10)\n",
        "    r.raise_for_status()\n",
        "    soup=BeautifulSoup(r.text,'html.parser')\n",
        "    h1=soup.find('h1'); headline=h1.get_text(strip=True) if h1 else 'n/a'\n",
        "    author='n/a'\n",
        "    for tag,cls in [('span','byline__name'),('p','metadata__byline__author')]:\n",
        "        el=soup.find(tag,class_=cls)\n",
        "        if el: author=el.get_text(strip=True); break\n",
        "    date = extract_date(soup)\n",
        "    paras = soup.find_all('div',class_='paragraph') or soup.find_all('div',class_='zn-body__paragraph') or soup.find_all('p')\n",
        "    content = \"\\n\".join(p.get_text(\" \",strip=True) for p in paras)\n",
        "    return {\"url\":url,\"headline\":headline,\"author\":author,\"date\":date,\"content\":content}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG7oNylg2R5s"
      },
      "source": [
        "# LLM call function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mU3z62U2YHB"
      },
      "outputs": [],
      "source": [
        "NVIDIA_INVOKE_URL = \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
        "def call_nvidia_llm(prompt):\n",
        "    resp = requests.post(\n",
        "      NVIDIA_INVOKE_URL,\n",
        "      headers={\"Authorization\":f\"Bearer {os.getenv('NVIDIA_API_KEY')}\",\"Accept\":\"application/json\"},\n",
        "      json={\n",
        "        \"model\":\"nvidia/nemotron-mini-4b-instruct\",\n",
        "        \"messages\":[{\"role\":\"user\",\"content\":prompt}],\n",
        "        \"max_tokens\":1024,\"temperature\":0.2,\"top_p\":0.7\n",
        "      }\n",
        "    )\n",
        "    resp.raise_for_status()\n",
        "    return resp.json()[\"choices\"][0][\"message\"][\"content\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ29fVwu2ZqK"
      },
      "source": [
        "# NeMo Embedder and Reranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyvDaRKDgmDf"
      },
      "outputs": [],
      "source": [
        "# embedder & reranker\n",
        "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embedqa-e5-v5\", truncate=\"END\")\n",
        "NV_rerank = NVIDIARerank(\n",
        "  model=\"nvidia/nv-rerankqa-mistral-4b-v3\",\n",
        "  api_key=os.getenv(\"NVIDIA_API_KEY\"),\n",
        "  top_n=20\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auvGMET92oZs"
      },
      "source": [
        "# Retriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKYo2PVagsA_"
      },
      "outputs": [],
      "source": [
        "# Retrieval helper\n",
        "KEYWORDS = [\"evacuate\",\"rescue\",\"firefighter\",\"fire\",\"water\",\"emergency\", \"firebreak\", \"emergency\", \"shelter\"]\n",
        "\n",
        "def retrieve_top_k(query, sentences, embeddings, k=50, boost=0.5, pre_k=100):\n",
        "    q_emb = np.array(embedder.embed_query(query))\n",
        "    sims = embeddings @ q_emb / (np.linalg.norm(embeddings,1) * np.linalg.norm(q_emb))\n",
        "    for i,s in enumerate(sentences):\n",
        "        if any(kw in s.lower() for kw in KEYWORDS): sims[i]+=boost\n",
        "    idxs=np.argsort(-sims)[:pre_k]\n",
        "    cands=[sentences[i] for i in idxs]\n",
        "    docs=[Document(page_content=t) for t in cands]\n",
        "    try:\n",
        "        out=NV_rerank.compress_documents(query=query,documents=docs)\n",
        "        reranked = [d.page_content for d in (out[\"documents\"] if isinstance(out,dict) else out)]\n",
        "    except:\n",
        "        reranked=cands\n",
        "    return reranked[:k]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PmBt54F2xTy"
      },
      "source": [
        "# SQL DB Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_u0Kv_fgfp5"
      },
      "outputs": [],
      "source": [
        "conn = sqlite3.connect(\"incidents.db\")\n",
        "c = conn.cursor()\n",
        "c.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS incidents (\n",
        "  id                          INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "  incident_name               TEXT,\n",
        "  incident_type               TEXT,\n",
        "  severity_level              INTEGER,\n",
        "  affected_location           TEXT,\n",
        "  identified_cause            TEXT,\n",
        "  estimated_economic_loss_usd INTEGER,\n",
        "  reported_fatalities         INTEGER,\n",
        "  reported_injuries           INTEGER,\n",
        "  incident_summary            TEXT,\n",
        "  response_measures           TEXT,\n",
        "  anticipated_developments    TEXT,\n",
        "  affected_population_estimate INTEGER,\n",
        "  evacuations_ordered         TEXT,\n",
        "  infrastructure_impacted     TEXT,\n",
        "  emergency_status            TEXT,\n",
        "  responding_agencies         TEXT\n",
        ")\n",
        "\"\"\")\n",
        "conn.commit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0NP77LZ20YG"
      },
      "source": [
        "# LLM summarization and Loading Data into SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhNLU3o1g3OG",
        "outputId": "68cebe90-318a-4666-c9e6-d4cd6af49e81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stored Oregon_Rowena_Fire_Jun2025 as id=1\n",
            "Stored Maui_Kahikinui_Fire_Jun2025 as id=2\n"
          ]
        }
      ],
      "source": [
        "# Process & store each incident\n",
        "for name, urls in incidents.items():\n",
        "    # scrape\n",
        "    arts = [scrape_article(u) for u in urls]\n",
        "    # build sentences+embeddings\n",
        "    sents=[]\n",
        "    for art in arts:\n",
        "        sents+= [t.strip() for t in re.split(r'(?<=[.?!])\\s+',art[\"content\"])\n",
        "                  if len(t.strip())>20 or re.search(r\"\\d\",t)]\n",
        "    embs = np.array(embedder.embed_documents(sents))\n",
        "    context=\"\\n\".join(retrieve_top_k(\n",
        "        \"Summarize the key facts and response measures\", sents, embs, k=50))\n",
        "    prompt=f\"\"\"\n",
        "You are an information‐extraction assistant.\n",
        "Extract only **explicitly stated** facts into JSON—do **not** infer, assume, or hallucinate.\n",
        "**Return only the JSON object, with no markdown fences, no explanations, and no extra text.**\n",
        "\n",
        "Here is the schema your output must follow exactly:\n",
        "{{\n",
        "  \"incident_name\" : \"\"                      // Must be the name of incident in short, sort of like a title, TEXT.\n",
        "  \"incident_type\": \"\",                      // Must be categorised into these categories only:  wildfire, urban fire, crowd-management incident, Undefined)\n",
        "  \"severity_level\": 0,                      // 1–10 reflecting severity based on described impact; use 0 if unspecified\n",
        "  \"affected_location\": \"\",                  // Region, city, state, etc.\n",
        "  \"identified_cause\": \"\",                   // Cause if mentioned (Look for text which mentions reason for any calamitites or incidents) or mention undefined\n",
        "  \"estimated_economic_loss_usd\": 0,         // Numeric, no formatting (e.g., 5000000)\n",
        "  \"reported_fatalities\": 0,                 // Number of deaths of people if any or 0\n",
        "  \"reported_injuries\": 0,                   // Number of injured of people if any or 0\n",
        "  \"incident_summary\": \"\",                   // 1–2 sentence summary of the event. Must highlight main points related mostly to the incident impact and emergency responses\n",
        "  \"response_measures\": \"\",                  // Steps taken in detail (e.g., evacuations, Incident-management, steps, declarations) by first responders or emergency agencies. Make sure to provide detailed information about the steps taken by the responders to handel the incident.\n",
        "                                            // Provide a bullet numbered list of steps taken by the authorities to manage the incident and also manage the calamity as we need to find how they contained it. Avoid descriptive and unrealted words in response.\n",
        "  \"anticipated_developments\": \"\",           // Future expectations or projections like if the threat is predicted to scale or it is mitigated\n",
        "  \"affected_population_estimate\": 0,        // Check if they have mentioned any number of people affected/evacuated or rescured only peoples Not the homes/property etc damaged etc., Only number of people. If not found mention 0\n",
        "  \"evacuations_ordered\": \"\",                // \"Yes\", \"No\", or \"n/a\"\n",
        "  \"infrastructure_impacted\": \"\",            // E.g., roads, power lines, water\n",
        "  \"emergency_status\": \"\",                   // \"Declared\", \"Not Declared\", or \"n/a\"\n",
        "  \"responding_agencies\": \"\",                // Government or official organizations mentioned\n",
        "}}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\".strip()\n",
        "    raw = call_nvidia_llm(prompt)\n",
        "    rec = json.loads(re.search(r\"\\{[\\s\\S]*\\}\",raw).group(0))\n",
        "    rec[\"incident_name\"]=name\n",
        "    cols=list(rec.keys())\n",
        "    ph=\",\".join(\"?\" for _ in cols)\n",
        "    sql=f\"INSERT INTO incidents ({','.join(cols)}) VALUES ({ph})\"\n",
        "    c.execute(sql,[rec[c] for c in cols])\n",
        "    conn.commit()\n",
        "    print(f\"Stored {name} as id={c.lastrowid}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Km8Up6yukTOx"
      },
      "outputs": [],
      "source": [
        "conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-RW3T2Etvbk6",
        "outputId": "deb866df-8c49-457e-818e-242731c0a577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atOJ6tT4veVD"
      },
      "source": [
        "# Vectorize SQL to DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIcTEtlvvbe-",
        "outputId": "ce7ea74a-41e6-42ae-efc6-86a29c66c662"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Loaded 2 incidents from SQLite.\n",
            "✅ FAISS index built with 2 vectors.\n"
          ]
        }
      ],
      "source": [
        "import sqlite3\n",
        "import numpy as np\n",
        "import faiss\n",
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
        "\n",
        "conn = sqlite3.connect(\"incidents.db\")\n",
        "cur  = conn.cursor()\n",
        "cur.execute(\"SELECT id, incident_summary, response_measures FROM incidents\")\n",
        "rows = cur.fetchall()\n",
        "conn.close()\n",
        "\n",
        "ids       = [r[0] for r in rows]\n",
        "documents = [f\"{r[1]} {r[2]}\" for r in rows]\n",
        "\n",
        "print(f\"Loaded {len(ids)} incidents from SQLite.\")\n",
        "\n",
        "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embedqa-e5-v5\", truncate=\"END\")\n",
        "vectors  = np.array(embedder.embed_documents(documents), dtype=\"float32\")\n",
        "\n",
        "d     = vectors.shape[1]\n",
        "index = faiss.IndexFlatIP(d)\n",
        "faiss.normalize_L2(vectors)\n",
        "index.add(vectors)\n",
        "\n",
        "print(f\"✅ FAISS index built with {index.ntotal} vectors.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_Egx-uExOTd",
        "outputId": "f277b3a8-0f7b-49a1-8473-0016ddbfa5e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"recommended_steps\": [\n",
            "    \"Implement a containment line to prevent the fire from spreading further.\",\n",
            "    \"Use water-dropping aircraft and helicopters to douse hotspots and slow the fire's spread.\",\n",
            "    \"Implement a backfiring strategy to create a firebreak and reduce the fuel load.\",\n",
            "    \"Use hand crews to build a fireline and maintain the containment line.\",\n",
            "    \"Monitor the fire's progress and adjust strategies as needed.\",\n",
            "    \"Communicate with local communities and provide evacuation orders if necessary.\",\n",
            "    \"Coordinate with other agencies and jurisdictions to share resources and information.\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def recommend_wildfire_control(query: str, top_k: int = 5):\n",
        "    q_vec = np.array(embedder.embed_query(query), dtype=\"float32\").reshape(1, -1)\n",
        "    faiss.normalize_L2(q_vec)\n",
        "    D, I = index.search(q_vec, top_k)\n",
        "    conn = sqlite3.connect(\"incidents.db\")\n",
        "    cur  = conn.cursor()\n",
        "    contexts = []\n",
        "    for idx in I[0]:\n",
        "        rec_id = ids[idx]\n",
        "        cur.execute(\n",
        "            \"SELECT incident_summary, response_measures FROM incidents WHERE id = ?\",\n",
        "            (rec_id,)\n",
        "        )\n",
        "        summary, measures = cur.fetchone()\n",
        "        contexts.append(f\"Summary: {summary}\\nMeasures: {measures}\")\n",
        "    conn.close()\n",
        "\n",
        "    # 4) Build the LLM prompt\n",
        "    combined_ctx = \"\\n\\n---\\n\\n\".join(contexts)\n",
        "    prompt = f\"\"\"\n",
        "You are an emergency response planning assistant helping the first responders and emergency agencies at the critical scene.\n",
        "Below are summaries and response measures from {top_k} past wildfire incidents:\n",
        "{combined_ctx}\n",
        "\n",
        "Given the new scenario:\n",
        "\\\"\\\"\\\"{query}\\\"\\\"\\\"\n",
        "\n",
        "Draft a JSON object with a key \"recommended_steps\" whose value is an array of concise, numbered best-practice steps to control this wildfire.\n",
        "Return **only** the JSON object, no extra text.\n",
        "\"\"\"\n",
        "    # 5) Call the NVIDIA LLM\n",
        "    raw = call_nvidia_llm(prompt)\n",
        "    # 6) Extract and parse JSON\n",
        "    m = re.search(r\"\\{{[\\s\\S]*\\}}\", raw)\n",
        "    js = m.group(0) if m else raw\n",
        "    return json.loads(js)\n",
        "\n",
        "# Example usage:\n",
        "query_text = \"There is a wildfire in California with heavy vegetation—what steps can be taken to control it?\"\n",
        "recommendations = recommend_wildfire_control(query_text, top_k=5)\n",
        "print(json.dumps(recommendations, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBLb43NKxOQE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LwqMGd57vbcW",
        "outputId": "14f96de3-2183-45a0-9167-0feff8a86135"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "Incident ID: 1  (score 0.376)\n",
            "Summary:\n",
            " A wildfire in Oregon prompted officials to issue evacuation orders for hundreds of homes and to close nearly 20 miles of an interstate in the Columbia River Gorge. \n",
            "\n",
            "Response Measures:\n",
            " A water helicopter and a plane dropping fire retardant helped fight the fire, which broke out on Wednesday. Firefighters from 10 states have come to Oregon to assist with suppression efforts. \n",
            "\n",
            "---\n",
            "Incident ID: 2  (score 0.356)\n",
            "Summary:\n",
            " Firefighters successfully contained the blaze, but 105 homes were evacuated due to strong winds and fast-moving fire. \n",
            "\n",
            "Response Measures:\n",
            " Firefighters, emergency officials, and the Hawaii National Guard deployed resources to battle the blaze. Evacuations were conducted, and part of a highway was closed. The American Red Cross provided shelters for displaced residents. \n",
            "\n",
            "---\n",
            "Incident ID: 2  (score -340282346638528859811704183484516925440.000)\n",
            "Summary:\n",
            " Firefighters successfully contained the blaze, but 105 homes were evacuated due to strong winds and fast-moving fire. \n",
            "\n",
            "Response Measures:\n",
            " Firefighters, emergency officials, and the Hawaii National Guard deployed resources to battle the blaze. Evacuations were conducted, and part of a highway was closed. The American Red Cross provided shelters for displaced residents. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# # Cell 9: Query the vector store for “control steps” advice\n",
        "\n",
        "# def get_control_steps(query: str, top_k: int = 3):\n",
        "#     # 1) Embed & normalize the query\n",
        "#     q_vec = np.array(embedder.embed_query(query), dtype=\"float32\").reshape(1, -1)\n",
        "#     faiss.normalize_L2(q_vec)\n",
        "#     # 2) Search FAISS\n",
        "#     D, I = index.search(q_vec, top_k)\n",
        "#     # 3) Retrieve and display the top matching incidents’ steps\n",
        "#     conn = sqlite3.connect(\"incidents.db\")\n",
        "#     cur  = conn.cursor()\n",
        "#     for score, idx in zip(D[0], I[0]):\n",
        "#         rec_id = ids[idx]\n",
        "#         cur.execute(\"SELECT incident_summary, response_measures FROM incidents WHERE id = ?\", (rec_id,))\n",
        "#         summary, measures = cur.fetchone()\n",
        "#         print(f\"---\\nIncident ID: {rec_id}  (score {score:.3f})\")\n",
        "#         print(\"Summary:\\n\", summary, \"\\n\")\n",
        "#         print(\"Response Measures:\\n\", measures, \"\\n\")\n",
        "#     conn.close()\n",
        "\n",
        "# # Example usage:\n",
        "# get_control_steps(\n",
        "#     \"There is a wildfire in California with heavy vegetation—what steps can be taken to control it?\"\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abRe9nTxvbZv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPiqnU8AvbXI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
